# -*- coding: utf-8 -*-
"""PlantDiseaseDetection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ROre955MpdtYGCMqtvvH9ASe75y07GZG
"""

!pip -q install kaggle

from google.colab import files
files.upload()  # ⬅️ choose kaggle.json from your computer

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d vipoooool/new-plant-diseases-dataset -p /content

!mkdir -p /content/plant_disease
!unzip -q /content/drive/MyDrive/archive.zip -d /content/plant_disease

"""##Importing Libraries"""

import matplotlib.pyplot as plt
import numpy as np
import os
import tensorflow as tf

"""##Data Preprocessing

###Training Image Preprocessing
"""

training_set = tf.keras.utils.image_dataset_from_directory(
    '/content/plant_disease/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/train',
    labels="inferred",
    label_mode="categorical",
    class_names=None,
    color_mode="rgb",
    batch_size=32,
    image_size=(128, 128),
    shuffle=True,
    seed=None,
    validation_split=None,
    subset=None,
    interpolation="bilinear",
    follow_links=False,
    crop_to_aspect_ratio=False,
    pad_to_aspect_ratio=False,
    verbose=True,
)

"""###Validation Image Preprocessing"""

validation_set = tf.keras.utils.image_dataset_from_directory(
    '/content/plant_disease/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/valid',
    labels="inferred",
    label_mode="categorical",
    class_names=None,
    color_mode="rgb",
    batch_size=32,
    image_size=(128, 128),
    shuffle=True,
    seed=None,
    validation_split=None,
    subset=None,
    interpolation="bilinear",
    follow_links=False,
    crop_to_aspect_ratio=False,
    pad_to_aspect_ratio=False,
    verbose=True,
)

for x,y in training_set.take(1):
  print(x,x.shape)
  print(y,y.shape)

"""##Building Model"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

"""###Building Convolution layer"""

model = Sequential()

model.add(Conv2D(32, (3, 3), activation='relu', padding= 'same', input_shape=(128, 128, 3)))
model.add(Conv2D(32, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size= 2, strides=2))

model.add(Conv2D(64, (3, 3), activation='relu', padding= 'same', input_shape=(128, 128, 3)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size= 2, strides=2))

model.add(Conv2D(128, (3, 3), activation='relu', padding= 'same', input_shape=(128, 128, 3)))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size= 2, strides=2))

model.add(Conv2D(256, (3, 3), activation='relu', padding= 'same', input_shape=(128, 128, 3)))
model.add(Conv2D(256, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size= 2, strides=2))

model.add(Conv2D(512, (3, 3), activation='relu', padding= 'same', input_shape=(128, 128, 3)))
model.add(Conv2D(512, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size= 2, strides=2))

model.add(Dropout(0.25)) #To avoid Overfitting

model.add(Flatten())

model.add(Dense(units=1500, activation='relu'))
model.add(Dense(units=720, activation='relu'))

model.add(Dropout(0.4))

##Output layer
model.add(Dense(units=38, activation='softmax'))

"""##Compiling Model"""

model.compile(optimizer=tf.keras.optimizers.Adam(
    learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])

model.summary()

training_history = model.fit(x=training_set, validation_data=validation_set, epochs=10)

"""##Model Evaluation"""

##Evaluating Model on training_set
train_acc, train_loss = model.evaluate(training_set)

print(train_acc, train_loss)

##model on validation set
val_acc, val_loss = model.evaluate(validation_set)

print(val_acc, val_loss)



"""##Saving Model"""

model.save("/content/plant_disease_detection_model.keras")      # Old H5 format

training_history.history

## Recording History
import json
with open('training_history.json', 'w') as f:
    json.dump(training_history.history, f)



"""##Accuracy Visualization"""

epochs = [i for  i in range(1,11)]
plt.plot(epochs,training_history.history['accuracy'], color='red', label='Training_Accuracy')
plt.plot(epochs,training_history.history['val_accuracy'], color='blue', label='validation_Accuracy')
# plt.yticks(np.arange(0.60, 1.01, 0.05))
plt.title('Training and Validation Accuracy')
plt.xlabel('No.of epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()



"""##Evaluating model over precision recall and F1 score"""

class_name = validation_set.class_names

class_name

test_set = tf.keras.utils.image_dataset_from_directory(
    '/content/plant_disease/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/valid',
    labels="inferred",
    label_mode="categorical",
    class_names=None,
    color_mode="rgb",
    batch_size=32,
    image_size=(128, 128),
    shuffle=False,
    seed=None,
    validation_split=None,
    subset=None,
    interpolation="bilinear",
    follow_links=False,
    crop_to_aspect_ratio=False,
    pad_to_aspect_ratio=False,
    verbose=True,
)

y_pred = model.predict(test_set)

y_pred.shape

predicted_categories = np.argmax(y_pred, axis=1)

true_categories = np.concatenate([y for x, y in test_set], axis=0)

Y_true = np.argmax(true_categories, axis=1)

Y_true

from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(Y_true,predicted_categories,target_names = class_name))

